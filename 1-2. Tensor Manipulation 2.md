# 1. View
 - element의 개수 자체는 변하지 않는다!
 - PyTorch의 View는 Numpy의 Reshape과 비슷한 역할을 한다.
 - View를 통해 텐서의 형태를 자유 자제로 바꿀 수 있다!
 - 2D / 3D 등을 구성 요소가 같게 자유 자제로 바꿀 수 있음.

```python
t = np.array([[[0, 1, 2],
               [3, 4, 5]],

              [[6, 7, 8],
               [9, 10, 11]]]) # 2 * 2 * 3의 텐서를 t에 넣는다.
ft = torch.FloatTensor(t) # float tensor에 넣고 ft에 넣는다.
print(ft.shape) # 2 * 2 * 3

print(ft.view([-1, 3])) # -1은 모르겠다는 뜻이다. 2 * 2 * 3을 ? * 3으로 맞추라는 뜻!
print(ft.view([-1, 3]).shape) # 2 * 2 * 3 => 4 * 3

print(ft.view([-1, 1, 3])) # -1은 모르겠다는 뜻이다. 2 * 2 * 3 을 ? * 1 * 3으로 맞추라는 뜻!
print(ft.view([-1, 1, 3]).shape) # 2 * 2 * 3 => 4 * 1 * 3
```

<br>
<hr>
<br>

# 2. Squeeze
 - element의 개수 자체는 변하지 않는다!
 - Squeeze는 쥐어 짜다는 뜻이다.
 - Squeeze를 통해 자동으로 Dimension의 요소가 1개인 경우 쥐어 짜서 없앤다!
 - squeeze(dim = ?)을 통해 해당 Dimension의 요소가 1개인 경우 쥐어 짤 수 있다.

```python
ft = torch.FloatTensor([[0], [1], [2]])
print(ft)
print(ft.shape) # 3 * 1

print(ft.squeeze()) # 요소가 하나씩 들어 있는 것을 쥐어 짜다!
print(ft.squeeze().shape) # 차원이 줄어 3이 됨
```

<br>
<hr>
<br>


# 3. Unsqueeze
 - element의 개수 자체는 변하지 않는다!
 - Squeeze를 반대로!
 - 원하는 자리에 차원을 추가해 준다.
 - Unsqueeze를 view로도 구현 가능하다.

```python
ft = torch.Tensor([0, 1, 2]) # 1차원 3개의 요소
print(ft.shape) # 3

print(ft.unsqueeze(0)) # dim을 0으로 설정
print(ft.unsqueeze(0).shape) # 0번째 자리에 차원을 추가해 준다. 3 => 1 * 3

# Unsqueeze를 view로도 구현 가능하다.
print(ft.view(1, -1)) # 3 * 1을 1 * ? 으로 맞춘다.
print(ft.view(1, -1).shape) # 1 * 3


print(ft.unsqueeze(1)) # dim을 1으로 설정
print(ft.unsqueeze(1).shape) # 1번째 자리에 차원을 추가해 준다. 3 => 3 * 1

print(ft.unsqueeze(-1)) # dim을 -1으로 설정 (마지막 디멘젼)
print(ft.unsqueeze(-1).shape) # 마지막 자리에 차원을 추가해 준다. 3 => 3 * 1
```

<br>
<hr>
<br>

# 4. Type Casting
 - 텐서의 타입을 바꿔 준다.

```python
lt = torch.LongTensor([1, 2, 3, 4]) # Long 타입
print(lt)

print(lt.float()) # float으로 바꿔줌.

bt = torch.ByteTensor([True, False, False, True]) # Boolean 타입
print(bt)

print(bt.long()) # long으로 바꿔줌.
print(bt.float()) # float으로 바꿔줌.
```

<br>
<hr>
<br>

# 5. Concatenation
 - Concat : 이어 붙이기
 
```python
x = torch.FloatTensor([[1, 2], [3, 4]]) # 2 * 2 크기
y = torch.FloatTensor([[5, 6], [7, 8]]) # 2 * 2 크기

print(torch.cat([x, y], dim=0)) # dim 0을 늘려라! cat[2 * 2, 2 * 2] => 4 * 2
print(torch.cat([x, y], dim=1)) # dim 1을 늘려라! cat[2 * 2, 2 * 2] => 2 * 4
```

<br>
<hr>
<br>

# 6. Stacking
 - Concat을 편리하게 이용하게 해준다. Concat을 단축시켜 이용함!
 - Unsqueeze -> cat의 과정을 통해서도 Stacking을 구현할 수 있다.

```python
x = torch.FloatTensor([1, 4]) # 2의 1D 벡터
y = torch.FloatTensor([2, 5]) # 2의 1D 벡터
z = torch.FloatTensor([3, 6]) # 2의 1D 벡터

print(torch.stack([x, y, z])) # x, y, z를 스택으로 쌓아라. (default는 dim = 0). dim 0을 늘려라! 3 * 2
print(torch.stack([x, y, z], dim=1)) # dim 1을 늘려라! 2 * 3

print(torch.cat([x.unsqueeze(0), y.unsqueeze(0), z.unsqueeze(0)], dim=0)) # Unsqueeze -> cat의 과정으로 Stacking 구현
# unsqueeze로 1 * 2를 3개 만들고 0 dimension을 기준으로 cat 한다.
```

<br>
<hr>
<br>

# 7. Ones and Zeros Like
 - 응용 : 같은 device에 선언하는 것이 중요하기 때문에, CPU 텐서와 GPU 텐서를 동시에 구동하면 에러가 나는데, 
 이 함수들을 이용하면 같은 device에 텐서들을 선언해 준다. (Multiple-CPU 등에서도 이용)

```python
x = torch.FloatTensor([[0, 1, 2], [2, 1, 0]]) # 2 * 3의 텐서
print(x)

print(torch.ones_like(x)) # x와 같은 사이즈의, 1의 요소로만 가득찬 텐서가 생성됨
print(torch.zeros_like(x)) # x와 같은 사이즈의, 0의 요소로만 가득찬 텐서가 생성됨
```

<br>
<hr>
<br>

# 8. In-place Operation
 - _를 통해 이용 가능!
 - 속도면에서 이점을 보기 위해 사용하지만, 속도 차이가 막 크게 나지는 않을 수 있음!

```python
x = torch.FloatTensor([[1, 2], [3, 4]])

print(x.mul(2.)) # [[2, 4], [6, 8]]. x에 2를 곱한다.
print(x) # [[1, 2], [3, 4]]
print(x.mul_(2.)) # [[2, 4], [6, 8]]. x에 2를 곱하는데, 메모리를 새로 선언하지 말고 기존의 텐서에 넣어라.
print(x) # [[2, 4], [6, 8]].
```
