# 1. Gradient Vanishing / Exploding
 - Gradient Vanishing : Gradient가 점차 작아지며 사라지는 것
 - Gradient Exploding : Gradient가 너무 커지는 것
 
# 2. Gradient Vanishing / Exploding 해결법
### (1) 간접적인 방법
 - 활성화 함수 바꾸기 (Sigmoid -> ReLU)
 - 초기화 잘하기
 - learning rate 작게 하기 : Exploding 해결법

<br>

### (2) 직접적인 방법
 - Batch Normalization
 
# 3. Batch Normalization 장점
 - 학습 과정 안정화
 - Regularization
 - 학습 속도 가속
 
# 4. Internal Covariate Shift
### (1) Covariate Shift
 - Covariate Shift는 Train set과 Test set의 분포 차이가 문제를 발생시킬 수 있다는 개념이다.
 - Covariate Shift는 입력과 출력의 분포 차이가 문제를 발생시킬 수 있다는 개념이다.
 - ex) 데이터를 학습 할 때는 실제 고양이들을 이용하고, Test 할 때 만화에 나오는 고양이를 준 다음 이게 고양이인지 확인해보라 하는 것이다. 이 예제에서는 Train set과 Test set의 분포가 2D와 3D, 사진과 그림, 색상 등 많은 부분에서 달랐다.
 
<br>

### (2) Internal Covariate Shift
 - Internal Covariate Shift가 Gradient Vanishing / Exploding의 원인이다.
 - 네트워크를 통한 학습 과정 중, 각 레이어에서 Covariate Shift가 발생하는 문제이다. 따라서 학습 전에 한 번만 정규화 한다고 해결되는 문제가 아니다!
 - 해결법 : Batch Normalization
 - ex) 고양이의 사진을 통해 최종적으로 고양이인지 아닌지 분류하는 NN 네트워크가 있다고 할 때, 
 Input인 고양이의 사진의 분포가 각 레이어를 지나며 학습되며 점차 분포가 변해 Output과 Input의 분포 차이가 발생한다.
