# 1. Softmax
 - Softmax Classification도 Logistic Regression의 연장선이다.
 - max 값을 얻고 싶은데, soft하게 얻고 깊은게 softmax이다.
 - Softmax 값들의 총 합은 1이다.
 - ex) (1, 2, 3)의 max 값을 (0.09, 0.24, 0.66)의 확률 값으로 찾는다!

# 2. Discrete Probability Distribution
 - Discrete Probability Distribution은 이산적인 확률 분포를 뜻한다.
 - 우리는 이산 확률 분포를 바탕으로 머신러닝을 수행하게 된다. 상대방이 '가위, 바위, 보'를 낼 확률을 예측한다고 하면, 이전의 패턴을 참고하게 된다. 확률 분포에 근사하기 위해 머신러닝을 이용한다.

# 3. Softmax code - Simple

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

torch.manual_seed(1) # 재현을 위함

z = torch.FloatTensor([1, 2, 3]) # 1, 2, 3

hypothesis = F.softmax(z, dim = 0) # 1, 2, 3 중 max를 soft하게 얻자.
print(hypothesis) # 0.09, 0.24, 0.66
hypothesis.sum() # 1. Softmax들의 총 합은 1


# 4. Cross Entropy
 - 두 개의 확률 분포가 주어졌을 때, 두 개가 얼마나 비슷한지 나타내는 수치.

# 3. Low-level Implementation

# 4. High-level Implementation

# 5. Training Example
