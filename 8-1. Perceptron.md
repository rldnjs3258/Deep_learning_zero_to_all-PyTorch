# 1. Neuron
 - 인공 신경망 : 인간의 뇌 속의 뉴런의 동작 과정을 본따 만든 모델이다.
 - 인간의 Neuron : 입력 -> Threshold 넘으면 활성화 되어 다음 Neuron으로 전파
 - 인공 신경망 (ANN)의 한 종류 중 Perceptron이 있다.
 
<br>
<hr>
<br>
 
# 2. Perceptron의 과정
 - (1) 입력 x가 들어 온다. 
 - (2) x에 가중치 W를 곱해서 x*W를 구하고 x*W들의 합과 bias를 더한다.
 - (3) Activation function(Sigmoid 등)을 거친다. (퍼셉트론은 하나의 레이어 만을 갖는다.)
 - (4) Output을 만든다.

 
<br>
<hr>
<br>


# 3. AND, OR
 - 초창기 Perceptron은 Linear classifier를 위해 만들어 졌다. (분류를 Linear 한 선으로 분류!)
 - Perceptron은 AND와 OR 문제를 해결하기 위해 나왔다!
 ### (1) AND
 - 둘 다 1일 경우에만 1이 나온다.
 - 아래의 표를 A와 B를 축으로 하는 그래프로 그리면 1이 Output인 경우만 linear하게 잘라서 분류할 수 있다.
 
 | A | B | Output |
 |---|:---:|---:|
 | 0 | 0 | 0 |
 | 0 | 1 | 0 |
 | 1 | 0 | 0 |
 | 1 | 1 | 1 |
 
 
 <br>
 
  ### (2) OR
 - 둘 중 하나만 1이어도 1이 나온다.
 - 아래의 표를 A와 B를 축으로 하는 그래프로 그리면 1이 Output인 경우들만 linear하게 잘라서 분류할 수 있다.
 
 
 | A | B | Output |
 |---|:---:|---:|
 | 0 | 0 | 0 |
 | 0 | 1 | 1 |
 | 1 | 0 | 1 |
 | 1 | 1 | 1 |


 
<br>
<hr>
<br>


# 4. XOR
 - 퍼셉트론으로는 XOR 문제를 linear하게 해결할 수 없.다. !!
 - 위와 같은 학술이 발표되면서 인공 신경망은 암흑기에 접어들고 주춤하게 된다.
 - XOR은 둘 중 하나만 1인 경우에만 1이 나온다.
 - 아래의 표를 A와 B를 축으로 하는 그래프로 그리면 1이 Output인 경우들만 linear하게 잘라서 분류할 수 없다!!!...
 
 
 | A | B | Output |
 |---|:---:|---:|
 | 0 | 0 | 0 |
 | 0 | 1 | 1 |
 | 1 | 0 | 1 |
 | 1 | 1 | 0 |

 
<br>
<hr>
<br>



# 5. 퍼셉트론 - XOR 실습 코드

```python
# XOR 데이터 정의
X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)
Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)

# 퍼셉트론 정의
linear = torch.nn.Linear(2, 1, bias = True) # 퍼셉트론은 1개의 레이어만을 갖기 때문에, torch.nn.Linear로 fully-connected layer를 선언한다.
sigmoid = torch.nn.Sigmoid() # activation function
model = torch.nn.sequential(linear, sigmoid).to(device)

criterion = torch.nn.BCELoss().to(device) # Binary classification이기 때문에 Binary Cross Entropy Loss(BCELoss)를 이용한다.
optimizer = torch.optim.SGD(model.parameters(), lr=1)

for step in range(10001): # 10000회 학습
  hypothesis = model(X) # 모델 퍼셉트론에 X를 넣어 예측값을 구한다.
  
  cost = criterion(hypothesis, Y) # 예측값과 실제 값 Y를 비교하여 cost를 구한다.

  optimizer.zero_grad()
  cost.backward()
  optimizer.step()
  
  if step % 100 == 0:
    print(step, cost.item())
    
# 결과
# 학습 이후 결과를 살펴보았는데, loss는 0.69314에서 안줄어 드는 것을 보아 퍼셉트론은 XOR을 제대로 학습하지 못하는 것을 알 수 있었다.
# 실제 값 [0], [1], [1], [0]을 출력해야 하는데 예측값은 [0.5], [0.5], [0.5], [0.5]인 것을 보아, 퍼셉트론은 XOR을 제대로 학습하지 못하는 것을 알 수 있었다.

```
