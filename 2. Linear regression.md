# 1. Data definition (데이터)
 - 공부한 시간과 점수의 상관관계를 학습시켜보자.
 - 모델을 학습시키기 위한 데이터는 torch.tensor의 형태를 띈다.

```python
x_train = torch.FloatTensor([[1], [2], [3]]) # 입력
y_train = torch.FloatTensor([[2], [4], [6]]) # 출력
```

<br>
<hr>
<br>

# 2. Hypothesis (모델 정의)
 - 우리의 모델이다. Hypothesis 구현하기!
 - y = Wx + b (W : Weight, b : Bias)
 - Linear regression : 학습 데이터와 가장 잘 맞는 직선을 찾는 일이다.
 - requires_grad = True : Pytorch에게 이 변수를 학습시키고 싶다고 명시.

```python
# Weight와 Bias 0으로 초기화 (어떤 입력이 들어와도 0을 예측하게 초기화!)
W = torch.zeros(1, requires_grad = True)
b = torch.zeros(1, requires_grad = True)
hypothesis = x_train * W + b
```

<br>
<hr>
<br>


# 3. Compute loss (학습)
 - 오류를 통해 지속적으로 모델을 개선시키는데, 이때 loss를 계산하는 것이 필요!
 ### (1) Cost(Loss) : 학습을 하려면 우리의 모델이 얼마나 정답과 가까운지 계산을 해야 한다.
  - Linear regression에서는 Mean Squared Error(MSE)로 loss를 계산한다.
  - MSE : 우리의 예측값과 실제 값의 차이의 제곱을 평균냄

```python
cost = torch.mean(hypothesis - y_train) ** 2 )
```

<br>
<hr>
<br>


# 4. Gradient descent (개선)
 - 계산한 loss를 통해 모델을 개선한다.
 1) Gradient descent 중 SGD의 방법을 써서 오류를 줄여가 보자.
 2) 우리의 모델에서 학습시키며 개선되어야 할 부분은 [W, b]이니까, 이 두개를 SGD에 넣자.
 3) 적당한 lr(학습율) 설정!
 4) zero_grad() : gradient 초기화
 5) backward() : gradient 계산
 6) step() : [W, b] 개선!

```python
optimizer = optim.SGD([W, b], lr = 0.01)

optimizer.zero_grad()
cost.backward()
optimizer.step()
```

<br>
<hr>
<br>

# 5. Full training code
 ### (1) 한 번만 정의!
  - 데이터 정의
  - Hypothesis 초기화
  - Optimizer 정의
  
 ### (2) 반복!
  - Hypothesis 예측
  - Cost 계산
  - Optimizer로 학습

```python
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[2], [4], [6]])

W = torch.zeros(1, requires_grad = True)
b = torch.zeros(1, requires_grad = True)

optimizer = optim.SGD([W, b], lr = 0.01)

nb_epochs = 1000 # 원하는 만큼 학습시키기!
for epoch in range(1, nb_epochs + 1):
    hypothesis = x_train * W + b # 모델로 예측
    cost = torch.mean(hypothesis - y_train) ** 2 ) # 오류율 계산

    # optimizer로 오류를 줄이기!
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
```
