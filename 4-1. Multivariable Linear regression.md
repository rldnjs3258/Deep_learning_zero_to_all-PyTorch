# 1. Simple Linear Regression 복습
 - 하나의 정보로부터 하나의 결론을 짓는 모델이다.
 - H(x) = Wx + b
 - ex) 공부 시간에 따른 시험 성적 예측
 

<br>
<hr>
<br>

# 2. Multivariate Linear Regression 이론
 - 복수의 정보로부터 하나의 결롯을 짓는 모델이다.
 - H(x) = w1x1 + w2x2 + w3x3 + b (입력 변수가 3개면 weight도 3개!)
 - ex) 첫번째 퀴즈 성적, 두번째 퀴즈 성적, 세번째 퀴즈 성적을 통해 final 시험을 예측한다.


<br>
<hr>
<br>

# 3. Naive Data Representation

### (1) 데이터 정의

```python
# 데이터
x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]]) # 첫번째 퀴즈 성적
x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]]) # 두번째 퀴즈 성적
x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]]) # 세번째 퀴즈 성적
y_train = torch.FloatTensor([[152], [185], [180], [196], [142]]) # final 시험
```

<br>

### (2) Hypothesis function 정의 - 기본
 - 아래와 같이 입력 변수의 수 만큼 weight를 곱했다.
 - 하.지.만. 입력 변수가 1000개라면..?

```python
hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b
```

<br>

### (3) Hypothesis function 정의 - matmul
 - matmul()로 한번에 정의가 가능하다. x의 길이가 바뀌어도 코드를 바꿀 필요가 없고, 속도도 더 빠르다.

```python
hypothesis = x_train.matmul(W) + b
```

<br>

### (4) 그 외
 - Cost function : MSE로 Linear regression과 동일!
 - Gradient Descent : SGD로 Linear regression과 동일!

<br>
<hr>
<br>

# 4. Full Code with torch.optim
 - Simple linear regression과 달라지는 부분은 데이터, W 밖에 없다.

```python
# 데이터
x_train = torch.FloatTensor([[73, 80, 75],
                             [93, 88, 93],
                             [89, 91, 90],
                             [96, 98, 100],
                             [73, 66, 70]])
y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])

# 모델 초기화
W = torch.zeros((3, 1), requires_grad=True) # 3개의 독립변수!
b = torch.zeros(1, requires_grad=True)

# optimizer 설정
optimizer = optim.SGD([W, b], lr=1e-5)

nb_epochs = 20
for epoch in range(nb_epochs + 1):
    
    # H(x) 계산
    hypothesis = x_train.matmul(W) + b

    # cost 계산
    cost = torch.mean((hypothesis - y_train) ** 2)

    # cost로 H(x) 개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    # 100번마다 로그 출력
    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(
        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()
    ))
```

<br>
<hr>
<br>

# 5. nn.Module 소개
 - 모델이 커질수록 W와 b를 일일히 치기 귀찮아 질 수 있다.

```python
import torch.nn as nn

class LinearRegressionModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(3, 1) # nn.Linear에 입력 차원과 출력 차원을 설정한다.

    def forward(self, x):
        return self.linear(x) # forward 함수에 hypothesis 계산을 어떻게 할지 설정한다.

hypothesis = model(x_train)
```

<br>
<hr>
<br>

# 6. F.mse_loss 소개
 - PyTorch에서는 다양한 cost(loss) function을 제공한다.
 - 원래 배웠던 것처럼 cost function을 직접 작성해도 되지만, 라이브러리를 이용하면 다른 loss function들과 쉽게 교체가 가능하다.
 - cost function 코드를 작성하면서 생길 수 있는 오류를 염려 안해도 된다.

```python
import torch.nn.functional as F

cost = F.mse_loss(prediction, y_train) # MSE!
```

<br>
<hr>
<br>


# 7. Full Code with torch.optim
 - nn.Module과 F.mse_loss를 적용한 코드!

```python
# 데이터
x_train = torch.FloatTensor([[73, 80, 75],
                             [93, 88, 93],
                             [89, 91, 90],
                             [96, 98, 100],
                             [73, 66, 70]])
y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])
# 모델 초기화
model = MultivariateLinearRegressionModel() # 변형됨!
# optimizer 설정
optimizer = optim.SGD(model.parameters(), lr=1e-5)

nb_epochs = 20
for epoch in range(nb_epochs+1):
    
    # H(x) 계산
    prediction = model(x_train) # 변형됨!
    
    # cost 계산
    cost = F.mse_loss(prediction, y_train) # 변형됨!
    
    # cost로 H(x) 개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
    
    # 20번마다 로그 출력
    print('Epoch {:4d}/{} Cost: {:.6f}'.format(
        epoch, nb_epochs, cost.item()
    ))
```
