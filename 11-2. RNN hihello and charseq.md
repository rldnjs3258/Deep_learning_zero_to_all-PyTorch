# 1. 'hihello' 예제
 - 'h -> i -> h -> e -> l -> l -> o'를 예측하자.

<br>
<hr>
<br>

# 2. 'hihello'를 데이터로 만들기 1 : 인덱스
 - 'hihello'를 컴퓨터가 이해할 수 있는 벡터로 만들어 보자.
 - 벡터화를 위해 단순히 '인덱스 순서'로 벡터를 만든다면? 컴퓨터가 데이터의 순서, 데이터가 나오는 빈도수를 파악하기 어렵다!
 - 'h' -> 0
 - 'i' -> 1
 - 'e' -> 2
 - 'l' -> 3
 - 'o' -> 4

```python
char_set = ['h', 'i', 'e', 'l', 'o']
```

<br>
<hr>
<br>


# 3. 'hihello'를 데이터로 만들기 2 : One-hot encoding
 - 'hihello'를 컴퓨터가 이해할 수 있는 벡터로 만들어 보자.
 - 벡터화를 위해 'One-hot encoding'을 이용한다면?
 - One-hot encoding : 특정 단어를 특정 인덱스로 치환한다. 

```python
char_set = ['h', 'i', 'e', 'l', 'o'] # 각 문자가 0, 1, 2, 3, 4의 인덱스로 치환된다.

# x_data는 문자열에서 맨 마지막 문자만 뺀 데이터이다.
x_data = [[0, 1, 0, 2, 3, 3]] # h, i, h, e, l, l
x_one_hot = [[[1, 0, 0, 0, 0], # h
              [0, 1, 0, 0, 0], # i
              [1, 0, 0, 0, 0], # h
              [0, 0, 1, 0, 0], # e
              [0, 0, 0, 1, 0], # l
              [0, 0, 0, 1, 0]]]# l
              
# y_data는 문자열에서 맨 처음 문자만 뺀 데이터이다.
y_data = [[1, 0, 2, 3, 3, 4]] # i, h, e, l, l, o
```

<br>
<hr>
<br>


# 4. Cross Entropy Loss
 - 일반적으로 Categorical한 예측을 해야 하는 경우, softmax를 이용하여 각 클래스가 될 확률을 통해 output을 구하게 된다.
 - Cross Entropy Loss는 Categorical 한 예측을 하는 모델에 주로 쓰이는 평가 방법이다.
 - ex) 인풋 x가 4개의 클래스로 분류 될 확률이 각각 [0.1, 0.2, 0.3, 0.4]이면 [0, 0, 0, 1]로 라벨링 되어 분류된다.

```python
# 사용할 loss 정의
criterion = torch.nn.CrossEntropyLoss()
# loss 사용
loss = criterion(outputs.view(-1, input_size), Y.view(-1)) # 첫 번째 파라미터는 모델의 예측 아웃풋, 두 번째 파라미터는 정답 레이블이다.
```

<br>
<hr>
<br>


# 5. hihello 데이터 준비하기

```python
# 문자 집합
char_set = ['h', 'i', 'e', 'l', 'o'] # 0, 1, 2, 3, 4

# 하이퍼 파라미터 설정
input_size = len(char_set)
hidden_size = len(char_set) # 꼭 이렇게 설정해야 하는 것 아님. 그냥 단순하게 input_size와 같게 설정한 것.
learning_rate = 0.1

# 데이터 정의
# x_data는 문자열에서 맨 마지막 문자만 뺀 데이터이다.
x_data = [[0, 1, 0, 2, 3, 3]] # h, i, h, e, l, l
x_one_hot = [[[1, 0, 0, 0, 0], # h
              [0, 1, 0, 0, 0], # i
              [1, 0, 0, 0, 0], # h
              [0, 0, 1, 0, 0], # e
              [0, 0, 0, 1, 0], # l
              [0, 0, 0, 1, 0]]]# l
              
# y_data는 문자열에서 맨 처음 문자만 뺀 데이터이다.              
y_data = [[1, 0, 2, 3, 4, 5]] # i, h, e, l, l, o

# 텐서화
X = torch.FloatTensor(x_one_hot) # 독립변수는 one hot encoding 된 것을 이용한다.
Y = torch.LongTensor(y_data) # 종속변수는 one hot encoding 되지 않아도 됨!
```

<br>
<hr>
<br>


# 6. Character Sequence(charseq) 데이터 준비하기
 - hihello 데이터와 크게 다를 바 없지만, 조금 더 일반화 된 코드이다.
 - sample을 어떤 문자열로 바꾸던 적용 가능한 코드이다! 

```python
sample = " if you want you"

# 문자 집합
char_set = list(set(sample)) # char_set은 character의 집합이다. set 함수로 sample에서 중복된 문자를 제거하고 유니크한 문자만 모아서 리스트로 만든다.
char_dic = {c: i for i, c in enumerate(char_set)) # char_dic은 특정 문자를 주면 그 인덱스를 찾아준다. enumerate를 쓰면 인덱스와 밸류를 같이 가져올 수 있다.

# 하이퍼 파라미터 설정
dic_size = len(char_dic)
hidden_size = len(char_dic)
learning_rate = 0.1

# 데이터 정의
sample_idx = [char_dic[c] for c in sample] # sample의 문자를 하나씩 가져와서 그 인덱스로 구성된 리스트를 만든다.

# x_data는 문자열에서 맨 마지막 문자만 뺀 데이터이다.
x_data = [sample_idx[:-1]]
x_one_hot = [np.eye(dic_size)[x] for x in x_data] # np.eye는 해당 인덱스의 위치는 1, 나머지는 0으로 만들어 주는 Identity Matrix 함수이다.

# y_data는 문자열에서 맨 처음 문자만 뺀 데이터이다.
y_data = [sample_idx[1:]]

# 텐서화
X = torch.FloatTensor(x_one_hot)
Y = torch.LongTensor(y_data)
```

<br>
<hr>
<br>


# 7. RNN 모델을 만들고 학습하기

```python
# RNN 모델 만들어 두기
rnn = torch.nn.RNN(input_size, hidden_size, batch_first = True) # batch_first = True로 지정하면 batch 디멘션이 가장 앞으로 온다.

# Loss와 Optimizer 정의
criterion = torch.nn.CrossEntropyLoss() # Loss
optimizer = optim.Adam(rnn.parameters(), learning_rate) # 최적화

# RNN 모델 학습하기!
for i in range(100):
  outputs, _status = rnn(X) 
  # outputs : 모델에 독립변수를 넣어 구한 예측값
  # _status : rnn 안에서 다음 셀로 넘어갈 때 계산하게 하는 hidden state
  
  loss = criterion(outputs.view(-1, input_size), Y.view(-1)) # (-1)로 batch dimension이 앞에 오도록 한 후, outputs와 Y로 loss를 구함

  optimizer.zero_grad() # 매번 새로 Gradient를 구하게 하기
  loss.backward() # BP로 Gradient 값을 구함
  optimizer.step() # parameter를 업데이트 함
  
  result = outputs.data.numpy().argmax(axis = 2) # outputs를 numpy 어레이로 가져 온다. axis = 2는 2번째 디멘션을 뜻하는데, 어떤 문자가 가장 가능성 있는지에 대한 숫자를 갖고 있다. argmax를 통해 그 중 가장 큰 값의 인덱스를 취한다.
  result_str = ''.join([char_set[c] for c in np.squeeze(result)]) # 앞에서 취한 인덱스가 어떠한 문자인지 char_set에서 확인한 후, join을 통해 문자열로 만든다. np.squeeze는 디멘션이 1인 축을 없애준다.
  print(i, "loss: ", loss.item(), "prediction: ", result, "true Y: ", y_data, "prediction str: ", result_str)
```
