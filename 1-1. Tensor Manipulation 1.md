# 1. Vector, Matrix and Tensor
### (1) Vector, Matrix and Tensor
 - 벡터 : 1차원으로 이루어진 행렬
 - 행렬 : 2차원으로 이루어진 행렬
 - 텐서 : 3차원으로 이루어진 행렬
 
 <br>
 
### (2) PyTorch Tensor Shape Convection
#### 1) 2D에서의 텐서
 - |t| = (batch size(세로) * dimension(가로))
 
 ex) |t| = batch size(64) * dim(256)
 
#### 2) 3D에서의 텐서
 - |t| = (batch size(세로) * width(가로) * height(깊이))
 
#### 3) 3D에서의 텐서 - NLP
 - |t| = (batch size(세로) * length(가로) * dim(깊이))
 - dim * length가 batch size만큼 쌓여 있다.
 
 <br>
 
<br>
<hr>
<br>

# 2. NumPy Review
 - NumPy와 PyTorch는 작성법이 아주 유사하고, 직관적이라서 이해가 쉽다.
### (1) Import
```
import numpy as np #넘파이
import torch #파이토치
```

<br>

### (2) 1D Array with NumPy
```
t = np.array([0., 1., 2., 3., 4., 5., 6.]) #0~6을 리스트로 선언하여 array로 만듦
print(t)

print('Rank of t: ', t.ndim) #t는 1차원 벡터
print('Shape of t: ', t.shape) #t에는 1차원이 7개 들어있음

print('t[0] t[1] t[-1] = ', t[0], t[1], t[-1]) #요소 확인
print('t[2:5] t[4:-1] = ', t[2:5], t[4:-1]) #Slicing으로 요소 확인
print('t[:2] t[3:] = ', t[:2], t[3:]) #Slicing으로 요소 확인
```

<br>

### (3) 2D Array with Numpy
```
t = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.], [10., 11., 12.]]) #2차원 리스트를 선언하여 array로 만듦
print(t)

print('Rank of t: ', t.ndim) #t는 2차원 행렬
print('Shape of t: ', t.shape) #t에는 4*3의 요소가 들어있음
```

<br>
<hr>
<br>

# 3. PyTorch Tensor Allocation
### (1) 1D Array with PyTorch
```
t = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.]) #1차원 리스트를 선언하여 floattensor로 만듦
print(t)

print(t.dim())  # 1차원
print(t.shape)  # 1차원 7개의 요소
print(t.size()) # 1차원 7개의 요소
print(t[0], t[1], t[-1])  # 요소 확인
print(t[2:5], t[4:-1])    # Slicing
print(t[:2], t[3:])       # Slicing
```

<br>

### (2) 2D Array with PyTorch
```
t = torch.FloatTensor([[1., 2., 3.],
                       [4., 5., 6.],
                       [7., 8., 9.],
                       [10., 11., 12.]
                      ]) #2차원 리스트를 선언하여 floattensor로 만듦
print(t)

print(t.dim())  # 2차원
print(t.size()) # 2차원 4*3 행렬
print(t[:, 1]) # 모든 행 중 첫번째 열만 찍어보기
print(t[:, 1].size()) # 크기
print(t[:, :-1]) # 모든 행 중 마지막 열 전까지만 직어보기
```

<br>

### (3) Broadcasting
 - 규칙 1 : 매트릭스 연산을 할 때는 두 텐서간의 크기가 같아야 한다.
 - 규칙 2 : 행렬 곱을 할 때 첫 번째 행렬의 마지막 차원과 두 번째 행렬의 첫번째 차원이 일치해야 한다.
 - 크기가 다른 텐서 간의 매트릭스 연산을 해야할 때, PyTorch는 Broadcasting 기능을 갖고 있어서 자동으로 크기를 맞춰서 계산하게 해준다.
 - 주의 : PyTorch가 '자동'으로 브로드캐스팅을 수행하기 때문에, 사용자는 행렬이 어떻게 바뀌었는지 왜 바뀌었는지 파악하지 못할 수 있고 브로드캐스팅을 쓰면 안 되는 때에 자동으로 변형될 수도 있다.
```
# Same shape
m1 = torch.FloatTensor([[3, 3]]) # 1*2 크기
m2 = torch.FloatTensor([[2, 2]]) # 1*2 크기
print(m1 + m2) #같은 크기이므로 계산 가능

# Vector + scalar (원래는 크기가 다르므로 연산 불가)
m1 = torch.FloatTensor([[1, 2]]) # 1*2 크기
m2 = torch.FloatTensor([3]) # 벡터. PyTorch는 3을 [[3, 3]]으로 바꿔 계산하게 해 준다.
print(m1 + m2)

# 2 x 1 Vector + 1 x 2 Vector (원래는 크기가 다르므로 연산 불가)
m1 = torch.FloatTensor([[1, 2]]) # 1*2 크기를 2*2로 자동으로 바꿔준다.
m2 = torch.FloatTensor([[3], [4]]) # 2*1 크기 2*2로 자동으로 바꿔준다.
print(m1 + m2)
```


<br>
<hr>
<br>

# 4. Matrix Multiplication
 - 딥러닝은 행렬곱을 굉장히 많이 수행한다.
 - 행렬곱에서는 첫번째 행렬의 마지막 차원과 두 번째 행렬의 첫번째 차원이 일치해야 계산 가능하다.
 크기가 다른 곳에서는 브로드캐스팅이 수행되어 계산 가능해진다.
```
print()
print('-------------')
print('Mul vs Matmul')
print('-------------')
m1 = torch.FloatTensor([[1, 2], [3, 4]])
m2 = torch.FloatTensor([[1], [2]])
print('Shape of Matrix 1: ', m1.shape) # 2 x 2
print('Shape of Matrix 2: ', m2.shape) # 2 x 1
print(m1.matmul(m2)) # 2 x 1

m1 = torch.FloatTensor([[1, 2], [3, 4]])
m2 = torch.FloatTensor([[1], [2]])
print('Shape of Matrix 1: ', m1.shape) # 2 * 2
print('Shape of Matrix 2: ', m2.shape) # 2 * 1 => 2 * 2 행렬곱 가능해짐!
print(m1 * m2) # 2 x 2
print(m1.mul(m2))
```

<br>
<hr>
<br>

# 5. Other Basic Ops
### (1) Mean
 - PyTorch에서는 평균을 구하는 법을 제공한다. NumPy에서의 방법과 굉장히 유용하다. (dim과 exc 부분만 차이가 있다.)

```
# 1차원 실수 행렬
t = torch.FloatTensor([1, 2]) #1차원 실수 행렬 선언
print(t.mean()) #mean 계산

# Can't use mean() on integers
t = torch.LongTensor([1, 2]) #1차원 long 행렬 선언. long 행렬에서는 mean 계산이 안될 때도 있다!
try:
    print(t.mean())
except Exception as exc:
    print(exc)

# 2차원 행렬
t = torch.FloatTensor([[1, 2], [3, 4]])
print(t)

print(t.mean()) # 모든 요소의 평균값
print(t.mean(dim=0)) # 행 방향의 dimension을 없앤다. [[1, 2,], [3, 4]] => mean [4, 6] = [2, 3]
print(t.mean(dim=1)) # 열 방향의 dimension을 없앤다. [[1, 2], [3, 4]] => mean [3, 7] = [1.5, 3.5]
print(t.mean(dim=-1))
```

<br>

### (2) Sum
```
t = torch.FloatTensor([[1, 2], [3, 4]])
print(t)

print(t.sum()) # 모든 요소를 더한 값
print(t.sum(dim=0)) # 행 방향의 dimension을 없앤다. sum [[1, 2], [3, 4]] => [4, 6]
print(t.sum(dim=1)) # 열 방향의 dimension을 없앤다. sum [[1, 2], [3, 4]] => [3, 7]
print(t.sum(dim=-1))
```

<br>

### (3) Max and Argmax
 - Max : 가장 큰 값을 찾는다.
 - Argmax : 가장 큰 값의 인덱스 값을 return한다.
```
t = torch.FloatTensor([[1, 2], [3, 4]])
print(t)

print(t.max()) # 가장 큰 값인 4 리턴

print(t.max(dim=0)) # Returns two values: max and argmax
print('Max: ', t.max(dim=0)[0]) # 행 방향의 dimension을 없애며 그 중 큰 값을 택한다. [[1, 2], [3, 4]] => [3, 4]
print('Argmax: ', t.max(dim=0)[1]) # 큰 값의 인덱스

print(t.max(dim=1)) # 열 방향의 dimension을 없애며 그 중 큰 값을 택한다. [[1, 2], [3, 4]] => [2, 4]
print(t.max(dim=-1))
```
